{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Teaching approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This course is being taught by Jeremy Howard, and was developed by Jeremy along with Rachel Thomas. Rachel has been dealing with a life-threatening illness so will not be teaching as originally planned this year.\n",
    "\n",
    "Jeremy has worked in a number of different areas - feel free to ask about anything that he might be able to help you with at any time, even if not directly related to the current topic:\n",
    "\n",
    "- Management consultant (McKinsey; AT Kearney)\n",
    "- Self-funded startup entrepreneur (Fastmail: first consumer synchronized email; Optimal Decisions: first optimized insurance pricing)\n",
    "- VC-funded startup entrepreneur: (Kaggle; Enlitic: first deep-learning medical company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I'll be using a *top-down* teaching method, which is different from how most math courses operate.  Typically, in a *bottom-up* approach, you first learn all the separate components you will be using, and then you gradually build them up into more complex structures.  The problems with this are that students often lose motivation, don't have a sense of the \"big picture\", and don't know what they'll need.\n",
    "\n",
    "If you took the fast.ai deep learning course, that is what we used.  You can hear more about my teaching philosophy [in this blog post](http://www.fast.ai/2016/10/08/teaching-philosophy/) or [in this talk](https://vimeo.com/214233053).\n",
    "\n",
    "Harvard Professor David Perkins has a book, [Making Learning Whole](https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719) in which he uses baseball as an analogy.  We don't require kids to memorize all the rules of baseball and understand all the technical details before we let them play the game.  Rather, they start playing with a just general sense of it, and then gradually learn more rules/details as time goes on.\n",
    "\n",
    "All that to say, don't worry if you don't understand everything at first!  You're not supposed to.  We will start using some \"black boxes\" such as random forests that haven't yet been explained in detail, and then we'll dig into the lower level details later.\n",
    "\n",
    "To start, focus on what things DO, not what they ARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Your practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "People learn by:\n",
    "1. **doing** (coding and building)\n",
    "2. **explaining** what they've learned (by writing or helping others)\n",
    "\n",
    "Therefore, we suggest that you practice these skills on Kaggle by:\n",
    "1. Entering competitions (*doing*)\n",
    "2. Creating Kaggle kernels (*explaining*)\n",
    "\n",
    "It's OK if you don't get good competition ranks or any kernel votes at first - that's totally normal! Just try to keep improving every day, and you'll see the results over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To get better at technical writing, study the top ranked Kaggle kernels from past competitions, and read posts from well-regarded technical bloggers. Some good role models include:\n",
    "\n",
    "- [Peter Norvig](http://nbviewer.jupyter.org/url/norvig.com/ipython/ProbabilityParadox.ipynb) (more [here](http://norvig.com/ipython/))\n",
    "- [Stephen Merity](https://smerity.com/articles/2017/deepcoder_and_ai_hype.html)\n",
    "- [Julia Evans](https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture) (more [here](https://jvns.ca/blog/2014/08/12/what-happens-if-you-write-a-tcp-stack-in-python/))\n",
    "- [Julia Ferraioli](http://blog.juliaferraioli.com/2016/02/exploring-world-using-vision-twilio.html)\n",
    "- [Edwin Chen](http://blog.echen.me/2014/10/07/moving-beyond-ctr-better-recommendations-through-human-evaluation/)\n",
    "- [Slav Ivanov](https://blog.slavv.com/picking-an-optimizer-for-style-transfer-86e7b8cba84b) (fast.ai student)\n",
    "- [Brad Kenstler](https://hackernoon.com/non-artistic-style-transfer-or-how-to-draw-kanye-using-captain-picards-face-c4a50256b814) (fast.ai and USF MSAN student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The more familiarity you have with numeric programming in Python, the better. If you're looking to improve in this area, we strongly suggest Wes McKinney's [Python for Data Analysis, 2nd ed](https://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1491957662/ref=asap_bc?ie=UTF8).\n",
    "\n",
    "For machine learning with Python, we recommend:\n",
    "\n",
    "- [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Andreas-Mueller/dp/1449369413): From one of the scikit-learn authors, which is the main library we'll be using\n",
    "- [Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow, 2nd Edition](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939/ref=dp_ob_title_bk): New version of a very successful book. A lot of the new material however covers deep learning in Tensorflow, which isn't relevant to this course\n",
    "- [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&psc=1&refRID=MBV2QMFH3EZ6B3YBY40K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syllabus in brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on time and class interests, we'll cover something like (not necessarily in this order):\n",
    "\n",
    "- Train vs test\n",
    "  - Effective validation set construction\n",
    "- Trees and ensembles\n",
    "  - Creating random forests\n",
    "  - Interpreting random forests\n",
    "- What is ML?  Why do we use it?\n",
    "  - What makes a good ML project?\n",
    "  - Structured vs unstructured data\n",
    "  - Examples of failures/mistakes\n",
    "- Feature engineering\n",
    "  - Domain specific - dates, URLs, text\n",
    "  - Embeddings / latent factors\n",
    "- Regularized models trained with SGD\n",
    "  - GLMs, Elasticnet, etc (NB: see what James covered)\n",
    "- Basic neural nets\n",
    "  - PyTorch\n",
    "  - Broadcasting, Matrix Multiplication\n",
    "  - Training loop, backpropagation\n",
    "- KNN\n",
    "- CV / bootstrap (Diabetes data set?)\n",
    "- Ethical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip:\n",
    "\n",
    "- Dimensionality reduction\n",
    "- Interactions\n",
    "- Monitoring training\n",
    "- Collaborative filtering\n",
    "- Momentum and LR annealing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import metrics\n",
    "from fastai.metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PATH = \"/media/bmn/NEMO/DATA/AI/numer.ai/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset-feb\t  example_predictions.csv  numerai_tournament_data.csv\r\n",
      "example_model.py  models\t\t   numerai_training_data.csv\r\n",
      "example_model.r   numerai_datasets.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to *Blue Book for Bulldozers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## About..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### ...our teaching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At fast.ai we have a distinctive [teaching philosophy](http://www.fast.ai/2016/10/08/teaching-philosophy/) of [\"the whole game\"](https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719/ref=sr_1_1?ie=UTF8&qid=1505094653).  This is different from how most traditional math & technical courses are taught, where you have to learn all the individual elements before you can combine them (Harvard professor David Perkins call this *elementitis*), but it is similar to how topics like *driving* and *baseball* are taught.  That is, you can start driving without [knowing how an internal combustion engine works](https://medium.com/towards-data-science/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153), and children begin playing baseball before they learn all the formal rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### ...our approach to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Most machine learning courses will throw at you dozens of different algorithms, with a brief technical description of the math behind them, and maybe a toy example. You're left confused by the enormous range of techniques shown and have little practical understanding of how to apply them.\n",
    "\n",
    "The good news is that modern machine learning can be distilled down to a couple of key techniques that are of very wide applicability. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods:\n",
    "\n",
    "- *Ensembles of decision trees* (i.e. Random Forests and Gradient Boosting Machines), mainly for structured data (such as you might find in a database table at most companies)\n",
    "- *Multi-layered neural networks learnt with SGD* (i.e. shallow and/or deep learning), mainly for unstructured data (such as audio, vision, and natural language)\n",
    "\n",
    "In this course we'll be doing a deep dive into random forests, and simple models learnt with SGD. You'll be learning about gradient boosting and deep learning in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ...this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will be looking at the Blue Book for Bulldozers Kaggle Competition: \"The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuaration.  The data is sourced from auction result postings and includes information on usage and equipment configurations.\"\n",
    "\n",
    "This is a very common type of dataset and prediciton problem, and similar to what you may see in your project or workplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ...Kaggle Competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Kaggle is an awesome resource for aspiring data scientists or anyone looking to improve their machine learning skills.  There is nothing like being able to get hands-on practice and receiving real-time feedback to help you improve your skills.\n",
    "\n",
    "Kaggle provides:\n",
    "\n",
    "1. Interesting data sets\n",
    "2. Feedback on how you're doing\n",
    "3. A leader board to see what's good, what's possible, and what's state-of-art.\n",
    "4. Blog posts by winning contestants share useful tips and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Kaggle provides info about some of the fields of our dataset; on the [Kaggle Data info](https://www.kaggle.com/c/bluebook-for-bulldozers/data) page they say the following:\n",
    "\n",
    "For this competition, you are predicting the sale price of bulldozers sold at auctions. The data for this competition is split into three parts:\n",
    "\n",
    "- **Train.csv** is the training set, which contains data through the end of 2011.\n",
    "- **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n",
    "- **Test.csv** is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n",
    "\n",
    "The key fields are in train.csv are:\n",
    "\n",
    "- SalesID: the uniue identifier of the sale\n",
    "- MachineID: the unique identifier of a machine.  A machine can be sold multiple times\n",
    "- saleprice: what the machine sold for at auction (only provided in train.csv)\n",
    "- saledate: the date of the sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*Question*\n",
    "\n",
    "What stands out to you from the above description?  What needs to be true of our training and validation sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.03 s, sys: 340 ms, total: 4.37 s\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%time df_raw = pd.read_csv(f'{PATH}numerai_training_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In any sort of data science work, it's **important to look at your data**, to make sure you understand the format, how it's stored, what type of values it holds, etc. Even if you've read descriptions about your data, the actual data may not be what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000): \n",
    "        with pd.option_context(\"display.max_columns\", 1000): \n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>393608</th>\n",
       "      <th>393609</th>\n",
       "      <th>393610</th>\n",
       "      <th>393611</th>\n",
       "      <th>393612</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>nf604a06ef5f4465</td>\n",
       "      <td>n326964b607134ce</td>\n",
       "      <td>ne86d8fb78540463</td>\n",
       "      <td>n03b66a8c3807405</td>\n",
       "      <td>n80ef0234cd0549a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>era</th>\n",
       "      <td>era120</td>\n",
       "      <td>era120</td>\n",
       "      <td>era120</td>\n",
       "      <td>era120</td>\n",
       "      <td>era120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_type</th>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature1</th>\n",
       "      <td>0.46443</td>\n",
       "      <td>0.14641</td>\n",
       "      <td>0.24786</td>\n",
       "      <td>0.72172</td>\n",
       "      <td>0.43593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature2</th>\n",
       "      <td>0.58762</td>\n",
       "      <td>0.79253</td>\n",
       "      <td>0.52512</td>\n",
       "      <td>0.34589</td>\n",
       "      <td>0.44446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature3</th>\n",
       "      <td>0.60014</td>\n",
       "      <td>0.64134</td>\n",
       "      <td>0.54871</td>\n",
       "      <td>0.43964</td>\n",
       "      <td>0.34415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature4</th>\n",
       "      <td>0.25017</td>\n",
       "      <td>0.43663</td>\n",
       "      <td>0.62041</td>\n",
       "      <td>0.33766</td>\n",
       "      <td>0.42438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature5</th>\n",
       "      <td>0.2843</td>\n",
       "      <td>0.38011</td>\n",
       "      <td>0.44319</td>\n",
       "      <td>0.58544</td>\n",
       "      <td>0.70391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature6</th>\n",
       "      <td>0.55571</td>\n",
       "      <td>0.19307</td>\n",
       "      <td>0.35409</td>\n",
       "      <td>0.57283</td>\n",
       "      <td>0.47222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature7</th>\n",
       "      <td>0.60684</td>\n",
       "      <td>0.39416</td>\n",
       "      <td>0.61332</td>\n",
       "      <td>0.83087</td>\n",
       "      <td>0.65756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature8</th>\n",
       "      <td>0.66328</td>\n",
       "      <td>0.65114</td>\n",
       "      <td>0.63423</td>\n",
       "      <td>0.76562</td>\n",
       "      <td>0.57509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature9</th>\n",
       "      <td>0.56966</td>\n",
       "      <td>0.41747</td>\n",
       "      <td>0.50896</td>\n",
       "      <td>0.48488</td>\n",
       "      <td>0.6294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature10</th>\n",
       "      <td>0.52987</td>\n",
       "      <td>0.57752</td>\n",
       "      <td>0.49379</td>\n",
       "      <td>0.34919</td>\n",
       "      <td>0.57127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature11</th>\n",
       "      <td>0.41602</td>\n",
       "      <td>0.45074</td>\n",
       "      <td>0.58368</td>\n",
       "      <td>0.56672</td>\n",
       "      <td>0.65658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature12</th>\n",
       "      <td>0.67824</td>\n",
       "      <td>0.4151</td>\n",
       "      <td>0.53804</td>\n",
       "      <td>0.66528</td>\n",
       "      <td>0.47602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature13</th>\n",
       "      <td>0.62255</td>\n",
       "      <td>0.50503</td>\n",
       "      <td>0.40894</td>\n",
       "      <td>0.72925</td>\n",
       "      <td>0.60386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature14</th>\n",
       "      <td>0.3358</td>\n",
       "      <td>0.67439</td>\n",
       "      <td>0.70189</td>\n",
       "      <td>0.36657</td>\n",
       "      <td>0.53437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature15</th>\n",
       "      <td>0.59167</td>\n",
       "      <td>0.40254</td>\n",
       "      <td>0.58522</td>\n",
       "      <td>0.75583</td>\n",
       "      <td>0.49007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature16</th>\n",
       "      <td>0.34632</td>\n",
       "      <td>0.32474</td>\n",
       "      <td>0.34445</td>\n",
       "      <td>0.44259</td>\n",
       "      <td>0.53114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature17</th>\n",
       "      <td>0.22361</td>\n",
       "      <td>0.33557</td>\n",
       "      <td>0.33245</td>\n",
       "      <td>0.37272</td>\n",
       "      <td>0.57177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature18</th>\n",
       "      <td>0.34705</td>\n",
       "      <td>0.21702</td>\n",
       "      <td>0.26757</td>\n",
       "      <td>0.63632</td>\n",
       "      <td>0.39891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature19</th>\n",
       "      <td>0.5288</td>\n",
       "      <td>0.63225</td>\n",
       "      <td>0.53232</td>\n",
       "      <td>0.53019</td>\n",
       "      <td>0.31867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature20</th>\n",
       "      <td>0.44527</td>\n",
       "      <td>0.25931</td>\n",
       "      <td>0.4765</td>\n",
       "      <td>0.45533</td>\n",
       "      <td>0.5498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature21</th>\n",
       "      <td>0.35273</td>\n",
       "      <td>0.62053</td>\n",
       "      <td>0.61085</td>\n",
       "      <td>0.42152</td>\n",
       "      <td>0.46444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature22</th>\n",
       "      <td>0.53149</td>\n",
       "      <td>0.29585</td>\n",
       "      <td>0.4514</td>\n",
       "      <td>0.82572</td>\n",
       "      <td>0.43516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature23</th>\n",
       "      <td>0.44162</td>\n",
       "      <td>0.53207</td>\n",
       "      <td>0.57987</td>\n",
       "      <td>0.53124</td>\n",
       "      <td>0.49486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature24</th>\n",
       "      <td>0.47459</td>\n",
       "      <td>0.56413</td>\n",
       "      <td>0.37245</td>\n",
       "      <td>0.40804</td>\n",
       "      <td>0.30872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature25</th>\n",
       "      <td>0.57393</td>\n",
       "      <td>0.76396</td>\n",
       "      <td>0.66278</td>\n",
       "      <td>0.47493</td>\n",
       "      <td>0.50539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature26</th>\n",
       "      <td>0.60562</td>\n",
       "      <td>0.39774</td>\n",
       "      <td>0.53148</td>\n",
       "      <td>0.56956</td>\n",
       "      <td>0.43225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature27</th>\n",
       "      <td>0.50619</td>\n",
       "      <td>0.65446</td>\n",
       "      <td>0.5431</td>\n",
       "      <td>0.53785</td>\n",
       "      <td>0.50585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature28</th>\n",
       "      <td>0.29066</td>\n",
       "      <td>0.49527</td>\n",
       "      <td>0.42693</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.54708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature29</th>\n",
       "      <td>0.42179</td>\n",
       "      <td>0.37159</td>\n",
       "      <td>0.27528</td>\n",
       "      <td>0.51351</td>\n",
       "      <td>0.32199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature30</th>\n",
       "      <td>0.3348</td>\n",
       "      <td>0.63026</td>\n",
       "      <td>0.60257</td>\n",
       "      <td>0.45284</td>\n",
       "      <td>0.33525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature31</th>\n",
       "      <td>0.27822</td>\n",
       "      <td>0.60695</td>\n",
       "      <td>0.56137</td>\n",
       "      <td>0.21678</td>\n",
       "      <td>0.49661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature32</th>\n",
       "      <td>0.53921</td>\n",
       "      <td>0.31124</td>\n",
       "      <td>0.39841</td>\n",
       "      <td>0.69689</td>\n",
       "      <td>0.29825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature33</th>\n",
       "      <td>0.5945</td>\n",
       "      <td>0.79252</td>\n",
       "      <td>0.6576</td>\n",
       "      <td>0.41104</td>\n",
       "      <td>0.39711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature34</th>\n",
       "      <td>0.70322</td>\n",
       "      <td>0.69593</td>\n",
       "      <td>0.65478</td>\n",
       "      <td>0.30334</td>\n",
       "      <td>0.52532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature35</th>\n",
       "      <td>0.58345</td>\n",
       "      <td>0.34205</td>\n",
       "      <td>0.39306</td>\n",
       "      <td>0.38948</td>\n",
       "      <td>0.59708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature36</th>\n",
       "      <td>0.50325</td>\n",
       "      <td>0.38312</td>\n",
       "      <td>0.43581</td>\n",
       "      <td>0.53597</td>\n",
       "      <td>0.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature37</th>\n",
       "      <td>0.47263</td>\n",
       "      <td>0.35047</td>\n",
       "      <td>0.26775</td>\n",
       "      <td>0.3806</td>\n",
       "      <td>0.63218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature38</th>\n",
       "      <td>0.62431</td>\n",
       "      <td>0.43007</td>\n",
       "      <td>0.44975</td>\n",
       "      <td>0.64302</td>\n",
       "      <td>0.51057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature39</th>\n",
       "      <td>0.40751</td>\n",
       "      <td>0.31461</td>\n",
       "      <td>0.47899</td>\n",
       "      <td>0.59013</td>\n",
       "      <td>0.41309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature40</th>\n",
       "      <td>0.53108</td>\n",
       "      <td>0.43322</td>\n",
       "      <td>0.29307</td>\n",
       "      <td>0.47325</td>\n",
       "      <td>0.51978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature41</th>\n",
       "      <td>0.4589</td>\n",
       "      <td>0.64917</td>\n",
       "      <td>0.51052</td>\n",
       "      <td>0.36443</td>\n",
       "      <td>0.40568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature42</th>\n",
       "      <td>0.54116</td>\n",
       "      <td>0.51942</td>\n",
       "      <td>0.52587</td>\n",
       "      <td>0.61422</td>\n",
       "      <td>0.49626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature43</th>\n",
       "      <td>0.35804</td>\n",
       "      <td>0.27353</td>\n",
       "      <td>0.28886</td>\n",
       "      <td>0.61932</td>\n",
       "      <td>0.61575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature44</th>\n",
       "      <td>0.42801</td>\n",
       "      <td>0.21498</td>\n",
       "      <td>0.24527</td>\n",
       "      <td>0.66691</td>\n",
       "      <td>0.4608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature45</th>\n",
       "      <td>0.8942</td>\n",
       "      <td>0.70129</td>\n",
       "      <td>0.77162</td>\n",
       "      <td>0.66464</td>\n",
       "      <td>0.56798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature46</th>\n",
       "      <td>0.60934</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.30768</td>\n",
       "      <td>0.6075</td>\n",
       "      <td>0.49451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature47</th>\n",
       "      <td>0.61506</td>\n",
       "      <td>0.58708</td>\n",
       "      <td>0.55382</td>\n",
       "      <td>0.49666</td>\n",
       "      <td>0.43709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature48</th>\n",
       "      <td>0.57353</td>\n",
       "      <td>0.35664</td>\n",
       "      <td>0.45543</td>\n",
       "      <td>0.6104</td>\n",
       "      <td>0.49284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature49</th>\n",
       "      <td>0.51455</td>\n",
       "      <td>0.43347</td>\n",
       "      <td>0.52726</td>\n",
       "      <td>0.44847</td>\n",
       "      <td>0.6222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature50</th>\n",
       "      <td>0.44337</td>\n",
       "      <td>0.56017</td>\n",
       "      <td>0.55424</td>\n",
       "      <td>0.43843</td>\n",
       "      <td>0.51929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     393608            393609            393610  \\\n",
       "id         nf604a06ef5f4465  n326964b607134ce  ne86d8fb78540463   \n",
       "era                  era120            era120            era120   \n",
       "data_type             train             train             train   \n",
       "feature1            0.46443           0.14641           0.24786   \n",
       "feature2            0.58762           0.79253           0.52512   \n",
       "feature3            0.60014           0.64134           0.54871   \n",
       "feature4            0.25017           0.43663           0.62041   \n",
       "feature5             0.2843           0.38011           0.44319   \n",
       "feature6            0.55571           0.19307           0.35409   \n",
       "feature7            0.60684           0.39416           0.61332   \n",
       "feature8            0.66328           0.65114           0.63423   \n",
       "feature9            0.56966           0.41747           0.50896   \n",
       "feature10           0.52987           0.57752           0.49379   \n",
       "feature11           0.41602           0.45074           0.58368   \n",
       "feature12           0.67824            0.4151           0.53804   \n",
       "feature13           0.62255           0.50503           0.40894   \n",
       "feature14            0.3358           0.67439           0.70189   \n",
       "feature15           0.59167           0.40254           0.58522   \n",
       "feature16           0.34632           0.32474           0.34445   \n",
       "feature17           0.22361           0.33557           0.33245   \n",
       "feature18           0.34705           0.21702           0.26757   \n",
       "feature19            0.5288           0.63225           0.53232   \n",
       "feature20           0.44527           0.25931            0.4765   \n",
       "feature21           0.35273           0.62053           0.61085   \n",
       "feature22           0.53149           0.29585            0.4514   \n",
       "feature23           0.44162           0.53207           0.57987   \n",
       "feature24           0.47459           0.56413           0.37245   \n",
       "feature25           0.57393           0.76396           0.66278   \n",
       "feature26           0.60562           0.39774           0.53148   \n",
       "feature27           0.50619           0.65446            0.5431   \n",
       "feature28           0.29066           0.49527           0.42693   \n",
       "feature29           0.42179           0.37159           0.27528   \n",
       "feature30            0.3348           0.63026           0.60257   \n",
       "feature31           0.27822           0.60695           0.56137   \n",
       "feature32           0.53921           0.31124           0.39841   \n",
       "feature33            0.5945           0.79252            0.6576   \n",
       "feature34           0.70322           0.69593           0.65478   \n",
       "feature35           0.58345           0.34205           0.39306   \n",
       "feature36           0.50325           0.38312           0.43581   \n",
       "feature37           0.47263           0.35047           0.26775   \n",
       "feature38           0.62431           0.43007           0.44975   \n",
       "feature39           0.40751           0.31461           0.47899   \n",
       "feature40           0.53108           0.43322           0.29307   \n",
       "feature41            0.4589           0.64917           0.51052   \n",
       "feature42           0.54116           0.51942           0.52587   \n",
       "feature43           0.35804           0.27353           0.28886   \n",
       "feature44           0.42801           0.21498           0.24527   \n",
       "feature45            0.8942           0.70129           0.77162   \n",
       "feature46           0.60934             0.494           0.30768   \n",
       "feature47           0.61506           0.58708           0.55382   \n",
       "feature48           0.57353           0.35664           0.45543   \n",
       "feature49           0.51455           0.43347           0.52726   \n",
       "feature50           0.44337           0.56017           0.55424   \n",
       "target                    1                 1                 1   \n",
       "\n",
       "                     393611            393612  \n",
       "id         n03b66a8c3807405  n80ef0234cd0549a  \n",
       "era                  era120            era120  \n",
       "data_type             train             train  \n",
       "feature1            0.72172           0.43593  \n",
       "feature2            0.34589           0.44446  \n",
       "feature3            0.43964           0.34415  \n",
       "feature4            0.33766           0.42438  \n",
       "feature5            0.58544           0.70391  \n",
       "feature6            0.57283           0.47222  \n",
       "feature7            0.83087           0.65756  \n",
       "feature8            0.76562           0.57509  \n",
       "feature9            0.48488            0.6294  \n",
       "feature10           0.34919           0.57127  \n",
       "feature11           0.56672           0.65658  \n",
       "feature12           0.66528           0.47602  \n",
       "feature13           0.72925           0.60386  \n",
       "feature14           0.36657           0.53437  \n",
       "feature15           0.75583           0.49007  \n",
       "feature16           0.44259           0.53114  \n",
       "feature17           0.37272           0.57177  \n",
       "feature18           0.63632           0.39891  \n",
       "feature19           0.53019           0.31867  \n",
       "feature20           0.45533            0.5498  \n",
       "feature21           0.42152           0.46444  \n",
       "feature22           0.82572           0.43516  \n",
       "feature23           0.53124           0.49486  \n",
       "feature24           0.40804           0.30872  \n",
       "feature25           0.47493           0.50539  \n",
       "feature26           0.56956           0.43225  \n",
       "feature27           0.53785           0.50585  \n",
       "feature28            0.2324           0.54708  \n",
       "feature29           0.51351           0.32199  \n",
       "feature30           0.45284           0.33525  \n",
       "feature31           0.21678           0.49661  \n",
       "feature32           0.69689           0.29825  \n",
       "feature33           0.41104           0.39711  \n",
       "feature34           0.30334           0.52532  \n",
       "feature35           0.38948           0.59708  \n",
       "feature36           0.53597             0.483  \n",
       "feature37            0.3806           0.63218  \n",
       "feature38           0.64302           0.51057  \n",
       "feature39           0.59013           0.41309  \n",
       "feature40           0.47325           0.51978  \n",
       "feature41           0.36443           0.40568  \n",
       "feature42           0.61422           0.49626  \n",
       "feature43           0.61932           0.61575  \n",
       "feature44           0.66691            0.4608  \n",
       "feature45           0.66464           0.56798  \n",
       "feature46            0.6075           0.49451  \n",
       "feature47           0.49666           0.43709  \n",
       "feature48            0.6104           0.49284  \n",
       "feature49           0.44847            0.6222  \n",
       "feature50           0.43843           0.51929  \n",
       "target                    1                 0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(df_raw.tail().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>393613</td>\n",
       "      <td>393613</td>\n",
       "      <td>n18f793b7b5154b8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>era</th>\n",
       "      <td>393613</td>\n",
       "      <td>120</td>\n",
       "      <td>era55</td>\n",
       "      <td>4041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_type</th>\n",
       "      <td>393613</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature1</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.467463</td>\n",
       "      <td>0.132239</td>\n",
       "      <td>0</td>\n",
       "      <td>0.37472</td>\n",
       "      <td>0.46674</td>\n",
       "      <td>0.55886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature2</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.457734</td>\n",
       "      <td>0.0992284</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39142</td>\n",
       "      <td>0.45725</td>\n",
       "      <td>0.52344</td>\n",
       "      <td>0.93888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature3</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.481866</td>\n",
       "      <td>0.116557</td>\n",
       "      <td>0</td>\n",
       "      <td>0.40229</td>\n",
       "      <td>0.47961</td>\n",
       "      <td>0.55935</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature4</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.444961</td>\n",
       "      <td>0.108713</td>\n",
       "      <td>0.01488</td>\n",
       "      <td>0.37228</td>\n",
       "      <td>0.44459</td>\n",
       "      <td>0.51676</td>\n",
       "      <td>0.91131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature5</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509518</td>\n",
       "      <td>0.113137</td>\n",
       "      <td>0</td>\n",
       "      <td>0.43495</td>\n",
       "      <td>0.51256</td>\n",
       "      <td>0.58712</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature6</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.484868</td>\n",
       "      <td>0.115371</td>\n",
       "      <td>0.01171</td>\n",
       "      <td>0.40467</td>\n",
       "      <td>0.48437</td>\n",
       "      <td>0.56348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature7</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.524472</td>\n",
       "      <td>0.123702</td>\n",
       "      <td>0</td>\n",
       "      <td>0.44128</td>\n",
       "      <td>0.52745</td>\n",
       "      <td>0.61054</td>\n",
       "      <td>0.9928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature8</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.558369</td>\n",
       "      <td>0.11497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.48223</td>\n",
       "      <td>0.56214</td>\n",
       "      <td>0.63851</td>\n",
       "      <td>0.98644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature9</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5175</td>\n",
       "      <td>0.101921</td>\n",
       "      <td>0</td>\n",
       "      <td>0.45102</td>\n",
       "      <td>0.51993</td>\n",
       "      <td>0.58638</td>\n",
       "      <td>0.96931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature10</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.469235</td>\n",
       "      <td>0.115823</td>\n",
       "      <td>0</td>\n",
       "      <td>0.38963</td>\n",
       "      <td>0.46503</td>\n",
       "      <td>0.54491</td>\n",
       "      <td>0.99205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature11</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.518194</td>\n",
       "      <td>0.10776</td>\n",
       "      <td>0</td>\n",
       "      <td>0.44782</td>\n",
       "      <td>0.52075</td>\n",
       "      <td>0.59104</td>\n",
       "      <td>0.98582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature12</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497087</td>\n",
       "      <td>0.103159</td>\n",
       "      <td>0.01745</td>\n",
       "      <td>0.42844</td>\n",
       "      <td>0.49696</td>\n",
       "      <td>0.56558</td>\n",
       "      <td>0.98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature13</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48978</td>\n",
       "      <td>0.116009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41285</td>\n",
       "      <td>0.48992</td>\n",
       "      <td>0.5671</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature14</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.506731</td>\n",
       "      <td>0.143601</td>\n",
       "      <td>0.00819</td>\n",
       "      <td>0.40406</td>\n",
       "      <td>0.50451</td>\n",
       "      <td>0.60782</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature15</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.540536</td>\n",
       "      <td>0.123725</td>\n",
       "      <td>0</td>\n",
       "      <td>0.45463</td>\n",
       "      <td>0.54341</td>\n",
       "      <td>0.62853</td>\n",
       "      <td>0.98328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature16</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.457506</td>\n",
       "      <td>0.0998048</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39005</td>\n",
       "      <td>0.45607</td>\n",
       "      <td>0.52359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature17</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.518136</td>\n",
       "      <td>0.1181</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4361</td>\n",
       "      <td>0.51318</td>\n",
       "      <td>0.59683</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature18</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.476602</td>\n",
       "      <td>0.129001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.38731</td>\n",
       "      <td>0.47859</td>\n",
       "      <td>0.56792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature19</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.539164</td>\n",
       "      <td>0.116479</td>\n",
       "      <td>0</td>\n",
       "      <td>0.45821</td>\n",
       "      <td>0.53724</td>\n",
       "      <td>0.61947</td>\n",
       "      <td>0.98506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature20</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.483003</td>\n",
       "      <td>0.0944112</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41934</td>\n",
       "      <td>0.48167</td>\n",
       "      <td>0.54556</td>\n",
       "      <td>0.94097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature21</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.522624</td>\n",
       "      <td>0.137896</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42789</td>\n",
       "      <td>0.52642</td>\n",
       "      <td>0.61947</td>\n",
       "      <td>0.98497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature22</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.540096</td>\n",
       "      <td>0.118153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.45975</td>\n",
       "      <td>0.54192</td>\n",
       "      <td>0.62183</td>\n",
       "      <td>0.99124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature23</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.532446</td>\n",
       "      <td>0.0946126</td>\n",
       "      <td>0</td>\n",
       "      <td>0.46912</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.59652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature24</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.455382</td>\n",
       "      <td>0.0949341</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39196</td>\n",
       "      <td>0.45441</td>\n",
       "      <td>0.51812</td>\n",
       "      <td>0.89898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature25</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.503964</td>\n",
       "      <td>0.116807</td>\n",
       "      <td>0.00393</td>\n",
       "      <td>0.42899</td>\n",
       "      <td>0.50925</td>\n",
       "      <td>0.58371</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature26</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.494822</td>\n",
       "      <td>0.095407</td>\n",
       "      <td>0.02969</td>\n",
       "      <td>0.43278</td>\n",
       "      <td>0.49643</td>\n",
       "      <td>0.55882</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature27</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.567138</td>\n",
       "      <td>0.0991953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50131</td>\n",
       "      <td>0.5687</td>\n",
       "      <td>0.63454</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature28</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.480337</td>\n",
       "      <td>0.111671</td>\n",
       "      <td>0</td>\n",
       "      <td>0.40176</td>\n",
       "      <td>0.47769</td>\n",
       "      <td>0.55586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature29</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.462148</td>\n",
       "      <td>0.117498</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3809</td>\n",
       "      <td>0.4555</td>\n",
       "      <td>0.53667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature30</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.512339</td>\n",
       "      <td>0.113778</td>\n",
       "      <td>0</td>\n",
       "      <td>0.43563</td>\n",
       "      <td>0.51335</td>\n",
       "      <td>0.59001</td>\n",
       "      <td>0.99262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature31</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495785</td>\n",
       "      <td>0.132275</td>\n",
       "      <td>0</td>\n",
       "      <td>0.40282</td>\n",
       "      <td>0.49315</td>\n",
       "      <td>0.58813</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature32</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.485137</td>\n",
       "      <td>0.110241</td>\n",
       "      <td>0.00536</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.48563</td>\n",
       "      <td>0.55918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature33</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.526166</td>\n",
       "      <td>0.125023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.44065</td>\n",
       "      <td>0.52877</td>\n",
       "      <td>0.61465</td>\n",
       "      <td>0.98449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature34</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.46142</td>\n",
       "      <td>0.11399</td>\n",
       "      <td>0</td>\n",
       "      <td>0.38327</td>\n",
       "      <td>0.46238</td>\n",
       "      <td>0.54055</td>\n",
       "      <td>0.9132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature35</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48256</td>\n",
       "      <td>0.12048</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39975</td>\n",
       "      <td>0.48125</td>\n",
       "      <td>0.5637</td>\n",
       "      <td>0.98227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature36</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.534419</td>\n",
       "      <td>0.097604</td>\n",
       "      <td>0</td>\n",
       "      <td>0.46954</td>\n",
       "      <td>0.5353</td>\n",
       "      <td>0.59958</td>\n",
       "      <td>0.97111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature37</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.521742</td>\n",
       "      <td>0.114922</td>\n",
       "      <td>0.04614</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.51997</td>\n",
       "      <td>0.59901</td>\n",
       "      <td>0.99486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature38</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.514272</td>\n",
       "      <td>0.113483</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.51435</td>\n",
       "      <td>0.59058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature39</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.496776</td>\n",
       "      <td>0.112781</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41931</td>\n",
       "      <td>0.49711</td>\n",
       "      <td>0.57387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature40</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.47676</td>\n",
       "      <td>0.109634</td>\n",
       "      <td>0</td>\n",
       "      <td>0.40118</td>\n",
       "      <td>0.47336</td>\n",
       "      <td>0.54897</td>\n",
       "      <td>0.98314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature41</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.528321</td>\n",
       "      <td>0.122839</td>\n",
       "      <td>0.03626</td>\n",
       "      <td>0.4443</td>\n",
       "      <td>0.5283</td>\n",
       "      <td>0.61275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature42</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495255</td>\n",
       "      <td>0.121439</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41339</td>\n",
       "      <td>0.49495</td>\n",
       "      <td>0.57697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature43</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.483345</td>\n",
       "      <td>0.12068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39796</td>\n",
       "      <td>0.47874</td>\n",
       "      <td>0.56401</td>\n",
       "      <td>0.99827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature44</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.488708</td>\n",
       "      <td>0.135982</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39524</td>\n",
       "      <td>0.48575</td>\n",
       "      <td>0.57976</td>\n",
       "      <td>0.99806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature45</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.532559</td>\n",
       "      <td>0.115896</td>\n",
       "      <td>0</td>\n",
       "      <td>0.45517</td>\n",
       "      <td>0.53637</td>\n",
       "      <td>0.61315</td>\n",
       "      <td>0.99359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature46</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53034</td>\n",
       "      <td>0.107398</td>\n",
       "      <td>0</td>\n",
       "      <td>0.45631</td>\n",
       "      <td>0.5285</td>\n",
       "      <td>0.60285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature47</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.519575</td>\n",
       "      <td>0.112527</td>\n",
       "      <td>0</td>\n",
       "      <td>0.44361</td>\n",
       "      <td>0.51944</td>\n",
       "      <td>0.59522</td>\n",
       "      <td>0.98335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature48</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.520685</td>\n",
       "      <td>0.116171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4411</td>\n",
       "      <td>0.52026</td>\n",
       "      <td>0.59969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature49</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.494798</td>\n",
       "      <td>0.109163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42123</td>\n",
       "      <td>0.49545</td>\n",
       "      <td>0.56921</td>\n",
       "      <td>0.96184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature50</th>\n",
       "      <td>393613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.521452</td>\n",
       "      <td>0.107282</td>\n",
       "      <td>0</td>\n",
       "      <td>0.44985</td>\n",
       "      <td>0.52216</td>\n",
       "      <td>0.59398</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>393613</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>196837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count  unique               top    freq      mean        std  \\\n",
       "id         393613  393613  n18f793b7b5154b8       1       NaN        NaN   \n",
       "era        393613     120             era55    4041       NaN        NaN   \n",
       "data_type  393613       1             train  393613       NaN        NaN   \n",
       "feature1   393613     NaN               NaN     NaN  0.467463   0.132239   \n",
       "feature2   393613     NaN               NaN     NaN  0.457734  0.0992284   \n",
       "feature3   393613     NaN               NaN     NaN  0.481866   0.116557   \n",
       "feature4   393613     NaN               NaN     NaN  0.444961   0.108713   \n",
       "feature5   393613     NaN               NaN     NaN  0.509518   0.113137   \n",
       "feature6   393613     NaN               NaN     NaN  0.484868   0.115371   \n",
       "feature7   393613     NaN               NaN     NaN  0.524472   0.123702   \n",
       "feature8   393613     NaN               NaN     NaN  0.558369    0.11497   \n",
       "feature9   393613     NaN               NaN     NaN    0.5175   0.101921   \n",
       "feature10  393613     NaN               NaN     NaN  0.469235   0.115823   \n",
       "feature11  393613     NaN               NaN     NaN  0.518194    0.10776   \n",
       "feature12  393613     NaN               NaN     NaN  0.497087   0.103159   \n",
       "feature13  393613     NaN               NaN     NaN   0.48978   0.116009   \n",
       "feature14  393613     NaN               NaN     NaN  0.506731   0.143601   \n",
       "feature15  393613     NaN               NaN     NaN  0.540536   0.123725   \n",
       "feature16  393613     NaN               NaN     NaN  0.457506  0.0998048   \n",
       "feature17  393613     NaN               NaN     NaN  0.518136     0.1181   \n",
       "feature18  393613     NaN               NaN     NaN  0.476602   0.129001   \n",
       "feature19  393613     NaN               NaN     NaN  0.539164   0.116479   \n",
       "feature20  393613     NaN               NaN     NaN  0.483003  0.0944112   \n",
       "feature21  393613     NaN               NaN     NaN  0.522624   0.137896   \n",
       "feature22  393613     NaN               NaN     NaN  0.540096   0.118153   \n",
       "feature23  393613     NaN               NaN     NaN  0.532446  0.0946126   \n",
       "feature24  393613     NaN               NaN     NaN  0.455382  0.0949341   \n",
       "feature25  393613     NaN               NaN     NaN  0.503964   0.116807   \n",
       "feature26  393613     NaN               NaN     NaN  0.494822   0.095407   \n",
       "feature27  393613     NaN               NaN     NaN  0.567138  0.0991953   \n",
       "feature28  393613     NaN               NaN     NaN  0.480337   0.111671   \n",
       "feature29  393613     NaN               NaN     NaN  0.462148   0.117498   \n",
       "feature30  393613     NaN               NaN     NaN  0.512339   0.113778   \n",
       "feature31  393613     NaN               NaN     NaN  0.495785   0.132275   \n",
       "feature32  393613     NaN               NaN     NaN  0.485137   0.110241   \n",
       "feature33  393613     NaN               NaN     NaN  0.526166   0.125023   \n",
       "feature34  393613     NaN               NaN     NaN   0.46142    0.11399   \n",
       "feature35  393613     NaN               NaN     NaN   0.48256    0.12048   \n",
       "feature36  393613     NaN               NaN     NaN  0.534419   0.097604   \n",
       "feature37  393613     NaN               NaN     NaN  0.521742   0.114922   \n",
       "feature38  393613     NaN               NaN     NaN  0.514272   0.113483   \n",
       "feature39  393613     NaN               NaN     NaN  0.496776   0.112781   \n",
       "feature40  393613     NaN               NaN     NaN   0.47676   0.109634   \n",
       "feature41  393613     NaN               NaN     NaN  0.528321   0.122839   \n",
       "feature42  393613     NaN               NaN     NaN  0.495255   0.121439   \n",
       "feature43  393613     NaN               NaN     NaN  0.483345    0.12068   \n",
       "feature44  393613     NaN               NaN     NaN  0.488708   0.135982   \n",
       "feature45  393613     NaN               NaN     NaN  0.532559   0.115896   \n",
       "feature46  393613     NaN               NaN     NaN   0.53034   0.107398   \n",
       "feature47  393613     NaN               NaN     NaN  0.519575   0.112527   \n",
       "feature48  393613     NaN               NaN     NaN  0.520685   0.116171   \n",
       "feature49  393613     NaN               NaN     NaN  0.494798   0.109163   \n",
       "feature50  393613     NaN               NaN     NaN  0.521452   0.107282   \n",
       "target     393613       2                 0  196837       NaN        NaN   \n",
       "\n",
       "               min      25%      50%      75%      max  \n",
       "id             NaN      NaN      NaN      NaN      NaN  \n",
       "era            NaN      NaN      NaN      NaN      NaN  \n",
       "data_type      NaN      NaN      NaN      NaN      NaN  \n",
       "feature1         0  0.37472  0.46674  0.55886        1  \n",
       "feature2         0  0.39142  0.45725  0.52344  0.93888  \n",
       "feature3         0  0.40229  0.47961  0.55935        1  \n",
       "feature4   0.01488  0.37228  0.44459  0.51676  0.91131  \n",
       "feature5         0  0.43495  0.51256  0.58712        1  \n",
       "feature6   0.01171  0.40467  0.48437  0.56348        1  \n",
       "feature7         0  0.44128  0.52745  0.61054   0.9928  \n",
       "feature8         0  0.48223  0.56214  0.63851  0.98644  \n",
       "feature9         0  0.45102  0.51993  0.58638  0.96931  \n",
       "feature10        0  0.38963  0.46503  0.54491  0.99205  \n",
       "feature11        0  0.44782  0.52075  0.59104  0.98582  \n",
       "feature12  0.01745  0.42844  0.49696  0.56558  0.98101  \n",
       "feature13        0  0.41285  0.48992   0.5671        1  \n",
       "feature14  0.00819  0.40406  0.50451  0.60782        1  \n",
       "feature15        0  0.45463  0.54341  0.62853  0.98328  \n",
       "feature16        0  0.39005  0.45607  0.52359        1  \n",
       "feature17        0   0.4361  0.51318  0.59683        1  \n",
       "feature18        0  0.38731  0.47859  0.56792        1  \n",
       "feature19        0  0.45821  0.53724  0.61947  0.98506  \n",
       "feature20        0  0.41934  0.48167  0.54556  0.94097  \n",
       "feature21        0  0.42789  0.52642  0.61947  0.98497  \n",
       "feature22        0  0.45975  0.54192  0.62183  0.99124  \n",
       "feature23        0  0.46912   0.5331  0.59652        1  \n",
       "feature24        0  0.39196  0.45441  0.51812  0.89898  \n",
       "feature25  0.00393  0.42899  0.50925  0.58371        1  \n",
       "feature26  0.02969  0.43278  0.49643  0.55882        1  \n",
       "feature27        0  0.50131   0.5687  0.63454        1  \n",
       "feature28        0  0.40176  0.47769  0.55586        1  \n",
       "feature29        0   0.3809   0.4555  0.53667        1  \n",
       "feature30        0  0.43563  0.51335  0.59001  0.99262  \n",
       "feature31        0  0.40282  0.49315  0.58813        1  \n",
       "feature32  0.00536   0.4116  0.48563  0.55918        1  \n",
       "feature33        0  0.44065  0.52877  0.61465  0.98449  \n",
       "feature34        0  0.38327  0.46238  0.54055   0.9132  \n",
       "feature35        0  0.39975  0.48125   0.5637  0.98227  \n",
       "feature36        0  0.46954   0.5353  0.59958  0.97111  \n",
       "feature37  0.04614   0.4429  0.51997  0.59901  0.99486  \n",
       "feature38        0   0.4375  0.51435  0.59058        1  \n",
       "feature39        0  0.41931  0.49711  0.57387        1  \n",
       "feature40        0  0.40118  0.47336  0.54897  0.98314  \n",
       "feature41  0.03626   0.4443   0.5283  0.61275        1  \n",
       "feature42        0  0.41339  0.49495  0.57697        1  \n",
       "feature43        0  0.39796  0.47874  0.56401  0.99827  \n",
       "feature44        0  0.39524  0.48575  0.57976  0.99806  \n",
       "feature45        0  0.45517  0.53637  0.61315  0.99359  \n",
       "feature46        0  0.45631   0.5285  0.60285        1  \n",
       "feature47        0  0.44361  0.51944  0.59522  0.98335  \n",
       "feature48        0   0.4411  0.52026  0.59969        1  \n",
       "feature49        0  0.42123  0.49545  0.56921  0.96184  \n",
       "feature50        0  0.44985  0.52216  0.59398        1  \n",
       "target         NaN      NaN      NaN      NaN      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(df_raw.describe(include='all').transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It's important to note what metric is being used for a project. Generally, selecting the metric(s) is an important part of the project setup. However, in this case Kaggle tells us what metric to use: RMSLE (root mean squared log error) between the actual and predicted auction prices. Therefore we take the log of the prices, so that RMSE will give us what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Initial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "??RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b10e8f610ee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'train'"
     ]
    }
   ],
   "source": [
    "m = RandomForestClassifier(n_jobs=-1, max_features=\"log2\", min_samples_leaf=3, oob_score=True)\n",
    "m.fit(df_raw.drop('target', axis=1), df_raw.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw.drop(['id', 'data_type'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This dataset contains a mix of **continuous** and **categorical** variables.\n",
    "\n",
    "The following method extracts particular date fields from a complete datetime for the purpose of constructing categoricals.  You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend/cyclical behavior as a function of time at any of these granularities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The categorical variables are currently stored as strings, which is inefficient, and doesn't provide the numeric coding required for a random forest. Therefore we call `train_cats` to convert strings to pandas categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_cats(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .cat accessor with a 'category' dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8665dd4b181c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mera\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3608\u001b[0m         if (name in self._internal_names_set or name in self._metadata or\n\u001b[1;32m   3609\u001b[0m                 name in self._accessors):\n\u001b[0;32m-> 3610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3611\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# this ensures that Series.str.<method> is well defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccessor_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__set__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/categorical.py\u001b[0m in \u001b[0;36m_make_accessor\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m   2209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2211\u001b[0;31m             raise AttributeError(\"Can only use .cat accessor with a \"\n\u001b[0m\u001b[1;32m   2212\u001b[0m                                  \"'category' dtype\")\n\u001b[1;32m   2213\u001b[0m         return CategoricalAccessor(data.values, data.index,\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .cat accessor with a 'category' dtype"
     ]
    }
   ],
   "source": [
    "df_raw.era = df_raw.era.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "5         0\n",
       "6         1\n",
       "7         0\n",
       "8         1\n",
       "9         1\n",
       "10        0\n",
       "11        1\n",
       "12        0\n",
       "13        1\n",
       "14        1\n",
       "15        0\n",
       "16        0\n",
       "17        1\n",
       "18        0\n",
       "19        1\n",
       "20        0\n",
       "21        0\n",
       "22        0\n",
       "23        1\n",
       "24        0\n",
       "25        0\n",
       "26        0\n",
       "27        0\n",
       "28        1\n",
       "29        0\n",
       "         ..\n",
       "393583    1\n",
       "393584    0\n",
       "393585    0\n",
       "393586    0\n",
       "393587    1\n",
       "393588    1\n",
       "393589    0\n",
       "393590    1\n",
       "393591    0\n",
       "393592    1\n",
       "393593    1\n",
       "393594    1\n",
       "393595    1\n",
       "393596    0\n",
       "393597    0\n",
       "393598    0\n",
       "393599    1\n",
       "393600    0\n",
       "393601    0\n",
       "393602    1\n",
       "393603    0\n",
       "393604    0\n",
       "393605    0\n",
       "393606    0\n",
       "393607    0\n",
       "393608    1\n",
       "393609    1\n",
       "393610    1\n",
       "393611    1\n",
       "393612    0\n",
       "Name: target, Length: 393613, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We're still not quite done - for instance we have lots of missing values, which we can't pass directly to a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "era          0.0\n",
       "feature1     0.0\n",
       "feature10    0.0\n",
       "feature11    0.0\n",
       "feature12    0.0\n",
       "feature13    0.0\n",
       "feature14    0.0\n",
       "feature15    0.0\n",
       "feature16    0.0\n",
       "feature17    0.0\n",
       "feature18    0.0\n",
       "feature19    0.0\n",
       "feature2     0.0\n",
       "feature20    0.0\n",
       "feature21    0.0\n",
       "feature22    0.0\n",
       "feature23    0.0\n",
       "feature24    0.0\n",
       "feature25    0.0\n",
       "feature26    0.0\n",
       "feature27    0.0\n",
       "feature28    0.0\n",
       "feature29    0.0\n",
       "feature3     0.0\n",
       "feature30    0.0\n",
       "feature31    0.0\n",
       "feature32    0.0\n",
       "feature33    0.0\n",
       "feature34    0.0\n",
       "feature35    0.0\n",
       "feature36    0.0\n",
       "feature37    0.0\n",
       "feature38    0.0\n",
       "feature39    0.0\n",
       "feature4     0.0\n",
       "feature40    0.0\n",
       "feature41    0.0\n",
       "feature42    0.0\n",
       "feature43    0.0\n",
       "feature44    0.0\n",
       "feature45    0.0\n",
       "feature46    0.0\n",
       "feature47    0.0\n",
       "feature48    0.0\n",
       "feature49    0.0\n",
       "feature5     0.0\n",
       "feature50    0.0\n",
       "feature6     0.0\n",
       "feature7     0.0\n",
       "feature8     0.0\n",
       "feature9     0.0\n",
       "target       0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(df_raw.isnull().sum().sort_index()/len(df_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But let's save this file for now, since it's already in format can be stored and accessed efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)\n",
    "df_raw.to_feather('tmp/numerai-feb-raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the future we can simply read it from this fast format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_feather('tmp/numerai-feb-raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df, y, nas = proc_df(df_raw, 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now have something we can pass to a random forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "??proc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmn/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/home/bmn/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9671352318139899"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RandomForestClassifier(n_jobs=-1, max_features=\"log2\", min_samples_leaf=3, oob_score=True)\n",
    "m.fit(df, y)\n",
    "m.score(df,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*todo* define r^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Wow, an r^2 of 0.98 - that's great, right? Well, perhaps not...\n",
    "\n",
    "Possibly **the most important idea** in machine learning is that of having separate training & validation data sets. As motivation, suppose you don't divide up your data, but instead use all of it.  And suppose you have lots of parameters:\n",
    "\n",
    "<img src=\"images/overfitting2.png\" alt=\"\" style=\"width: 70%\"/>\n",
    "<center>\n",
    "[Underfitting and Overfitting](https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted)\n",
    "</center>\n",
    "\n",
    "The error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it's not the best choice.  Why is that?  If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\n",
    "\n",
    "This illustrates how using all our data can lead to **overfitting**. A validation set helps diagnose this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((381613, 51), (381613,), (12000, 51))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "n_valid = 12000  # same as Kaggle's test set size\n",
    "n_trn = len(df)-n_valid\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "X_train, X_valid = split_vals(df, n_trn)\n",
    "y_train, y_valid = split_vals(y, n_trn)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try our model again, this time with separate training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def accuracy_np(probs, y):\n",
    "    preds = np.argmax(probs)\n",
    "    return (preds==y).mean()\n",
    "\n",
    "def fun(x, y):\n",
    "    log_preds = np.log(x)\n",
    "    probs = np.mean(np.exp(log_preds),0)\n",
    "    return accuracy_np(probs, y)\n",
    "\n",
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    res = [fun(m.predict(X_train), y_train), fun(m.predict(X_valid), y_valid),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 23.8 ms, total: 11.6 s\n",
      "Wall time: 1.92 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmn/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50005, 0.5008333333333334, 0.9996, 0.5015833333333334, 0.5038]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestClassifier(n_estimators=40, n_jobs=-1, max_features=\"auto\", min_samples_leaf=3, oob_score=True)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An r^2 in the high-80's isn't bad at all (and the RMSLE puts us around rank 100 of 470 on the Kaggle leaderboard), but we can see from the validation set score that we're over-fitting badly. To understand this issue, let's simplify things down to a single small tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Speeding things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_trn, y_trn, nas = proc_df(df_raw, 'target', subset=30000, na_dict=nas)\n",
    "X_train, _ = split_vals(df_trn, 20000)\n",
    "y_train, _ = split_vals(y_trn, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 12.1 ms, total: 15.5 s\n",
      "Wall time: 2.92 s\n",
      "[0.22216660415102898, 0.5262136764978019, 0.8025679980256799, -0.10760641001780535]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49866268608228664, 0.5013782155593751, 0.005341107377902721, -0.00552325327016101]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"720pt\" height=\"434pt\"\n",
       " viewBox=\"0.00 0.00 720.00 434.49\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.778659 0.778659) rotate(0) translate(4 554)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-554 920.667,-554 920.667,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.772549\" stroke=\"black\" points=\"151.667,-308.5 37.6667,-308.5 37.6667,-240.5 151.667,-240.5 151.667,-308.5\"/>\n",
       "<text text-anchor=\"start\" x=\"45.6667\" y=\"-293.3\" font-family=\"Times,serif\" font-size=\"14.00\">feature37 ≤ 0.557</text>\n",
       "<text text-anchor=\"start\" x=\"63.1667\" y=\"-278.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.25</text>\n",
       "<text text-anchor=\"start\" x=\"47.6667\" y=\"-263.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 20000</text>\n",
       "<text text-anchor=\"start\" x=\"63.1667\" y=\"-248.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.5</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.796078\" stroke=\"black\" points=\"399.667,-361.5 285.667,-361.5 285.667,-293.5 399.667,-293.5 399.667,-361.5\"/>\n",
       "<text text-anchor=\"start\" x=\"293.667\" y=\"-346.3\" font-family=\"Times,serif\" font-size=\"14.00\">feature25 ≤ 0.606</text>\n",
       "<text text-anchor=\"start\" x=\"311.167\" y=\"-331.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.25</text>\n",
       "<text text-anchor=\"start\" x=\"295.667\" y=\"-316.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 12496</text>\n",
       "<text text-anchor=\"start\" x=\"304.667\" y=\"-301.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.512</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M151.796,-286.593C188.505,-294.502 236.63,-304.87 275.244,-313.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"274.841,-316.683 285.353,-315.367 276.315,-309.84 274.841,-316.683\"/>\n",
       "<text text-anchor=\"middle\" x=\"264.349\" y=\"-325.225\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.729412\" stroke=\"black\" points=\"396.167,-255.5 289.167,-255.5 289.167,-187.5 396.167,-187.5 396.167,-255.5\"/>\n",
       "<text text-anchor=\"start\" x=\"297.167\" y=\"-240.3\" font-family=\"Times,serif\" font-size=\"14.00\">feature2 ≤ 0.627</text>\n",
       "<text text-anchor=\"start\" x=\"311.167\" y=\"-225.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.25</text>\n",
       "<text text-anchor=\"start\" x=\"298.667\" y=\"-210.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 7504</text>\n",
       "<text text-anchor=\"start\" x=\"304.667\" y=\"-195.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.481</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M151.796,-262.407C189.757,-254.229 239.927,-243.42 279.162,-234.967\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"280.105,-238.344 289.144,-232.816 278.631,-231.501 280.105,-238.344\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.14\" y=\"-215.558\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.776471\" stroke=\"black\" points=\"642.667,-486.5 538.667,-486.5 538.667,-418.5 642.667,-418.5 642.667,-486.5\"/>\n",
       "<text text-anchor=\"start\" x=\"548.167\" y=\"-471.3\" font-family=\"Times,serif\" font-size=\"14.00\">feature9 ≤ 0.52</text>\n",
       "<text text-anchor=\"start\" x=\"559.167\" y=\"-456.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.25</text>\n",
       "<text text-anchor=\"start\" x=\"546.667\" y=\"-441.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 9899</text>\n",
       "<text text-anchor=\"start\" x=\"552.667\" y=\"-426.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.503</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M399.796,-356.021C438.392,-375.633 489.61,-401.658 529.123,-421.736\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"527.888,-425.034 538.388,-426.444 531.059,-418.794 527.888,-425.034\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.874510\" stroke=\"black\" points=\"647.667,-361.5 533.667,-361.5 533.667,-293.5 647.667,-293.5 647.667,-361.5\"/>\n",
       "<text text-anchor=\"start\" x=\"541.667\" y=\"-346.3\" font-family=\"Times,serif\" font-size=\"14.00\">feature25 ≤ 0.766</text>\n",
       "<text text-anchor=\"start\" x=\"555.667\" y=\"-331.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.248</text>\n",
       "<text text-anchor=\"start\" x=\"546.667\" y=\"-316.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 2597</text>\n",
       "<text text-anchor=\"start\" x=\"552.667\" y=\"-301.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.547</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M399.796,-327.5C436.505,-327.5 484.63,-327.5 523.244,-327.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"523.353,-331 533.353,-327.5 523.353,-324 523.353,-331\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.733333\" stroke=\"black\" points=\"882.667,-550 778.667,-550 778.667,-497 882.667,-497 882.667,-550\"/>\n",
       "<text text-anchor=\"start\" x=\"799.167\" y=\"-534.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.25</text>\n",
       "<text text-anchor=\"start\" x=\"786.667\" y=\"-519.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 4815</text>\n",
       "<text text-anchor=\"start\" x=\"792.667\" y=\"-504.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.483</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M642.724,-467.731C679.617,-478.737 729.613,-493.652 768.619,-505.288\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"767.701,-508.667 778.284,-508.172 769.702,-501.959 767.701,-508.667\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.819608\" stroke=\"black\" points=\"882.667,-479 778.667,-479 778.667,-426 882.667,-426 882.667,-479\"/>\n",
       "<text text-anchor=\"start\" x=\"799.167\" y=\"-463.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.25</text>\n",
       "<text text-anchor=\"start\" x=\"786.667\" y=\"-448.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 5084</text>\n",
       "<text text-anchor=\"start\" x=\"792.667\" y=\"-433.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.522</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M642.724,-452.5C679.461,-452.5 729.189,-452.5 768.122,-452.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"768.284,-456 778.284,-452.5 768.284,-449 768.284,-456\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.898039\" stroke=\"black\" points=\"882.667,-408 778.667,-408 778.667,-355 882.667,-355 882.667,-408\"/>\n",
       "<text text-anchor=\"start\" x=\"795.667\" y=\"-392.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.247</text>\n",
       "<text text-anchor=\"start\" x=\"786.667\" y=\"-377.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 2440</text>\n",
       "<text text-anchor=\"start\" x=\"792.667\" y=\"-362.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.557</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M647.768,-340.229C683.946,-348.437 731.008,-359.115 768.198,-367.553\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"767.882,-371.071 778.409,-369.87 769.431,-364.244 767.882,-371.071\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.525490\" stroke=\"black\" points=\"879.167,-337 782.167,-337 782.167,-284 879.167,-284 879.167,-337\"/>\n",
       "<text text-anchor=\"start\" x=\"795.667\" y=\"-321.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.238</text>\n",
       "<text text-anchor=\"start\" x=\"790.167\" y=\"-306.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 157</text>\n",
       "<text text-anchor=\"start\" x=\"792.667\" y=\"-291.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.389</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M647.768,-323.493C685.076,-320.828 733.96,-317.336 771.655,-314.644\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"772.246,-318.11 781.972,-313.907 771.748,-311.128 772.246,-318.11\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.721569\" stroke=\"black\" points=\"647.667,-255.5 533.667,-255.5 533.667,-187.5 647.667,-187.5 647.667,-255.5\"/>\n",
       "<text text-anchor=\"start\" x=\"541.667\" y=\"-240.3\" font-family=\"Times,serif\" font-size=\"14.00\">feature23 ≤ 0.508</text>\n",
       "<text text-anchor=\"start\" x=\"555.667\" y=\"-225.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.249</text>\n",
       "<text text-anchor=\"start\" x=\"546.667\" y=\"-210.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 7176</text>\n",
       "<text text-anchor=\"start\" x=\"552.667\" y=\"-195.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.477</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M396.44,-221.5C433.583,-221.5 483.576,-221.5 523.404,-221.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"523.559,-225 533.559,-221.5 523.559,-218 523.559,-225\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.937255\" stroke=\"black\" points=\"647.667,-131.5 533.667,-131.5 533.667,-63.5 647.667,-63.5 647.667,-131.5\"/>\n",
       "<text text-anchor=\"start\" x=\"541.667\" y=\"-116.3\" font-family=\"Times,serif\" font-size=\"14.00\">feature40 ≤ 0.692</text>\n",
       "<text text-anchor=\"start\" x=\"555.667\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.244</text>\n",
       "<text text-anchor=\"start\" x=\"550.167\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 328</text>\n",
       "<text text-anchor=\"start\" x=\"552.667\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.576</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M396.44,-194.899C433.899,-176.017 484.426,-150.548 524.417,-130.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"526.205,-133.408 533.559,-125.782 523.054,-127.158 526.205,-133.408\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.768627\" stroke=\"black\" points=\"882.667,-266 778.667,-266 778.667,-213 882.667,-213 882.667,-266\"/>\n",
       "<text text-anchor=\"start\" x=\"799.167\" y=\"-250.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.25</text>\n",
       "<text text-anchor=\"start\" x=\"786.667\" y=\"-235.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 3463</text>\n",
       "<text text-anchor=\"start\" x=\"792.667\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.499</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M647.768,-225.743C683.946,-228.479 731.008,-232.038 768.198,-234.851\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"768.173,-238.359 778.409,-235.623 768.701,-231.379 768.173,-238.359\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\n",
       "<polygon fill=\"#e58139\" fill-opacity=\"0.674510\" stroke=\"black\" points=\"882.667,-195 778.667,-195 778.667,-142 882.667,-142 882.667,-195\"/>\n",
       "<text text-anchor=\"start\" x=\"795.667\" y=\"-179.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.248</text>\n",
       "<text text-anchor=\"start\" x=\"786.667\" y=\"-164.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 3713</text>\n",
       "<text text-anchor=\"start\" x=\"792.667\" y=\"-149.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.456</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M647.768,-209.007C683.946,-200.95 731.008,-190.47 768.198,-182.188\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"769.409,-185.505 778.409,-179.915 767.887,-178.672 769.409,-185.505\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\n",
       "<polygon fill=\"#e58139\" stroke=\"black\" points=\"879.167,-124 782.167,-124 782.167,-71 879.167,-71 879.167,-124\"/>\n",
       "<text text-anchor=\"start\" x=\"795.667\" y=\"-108.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.239</text>\n",
       "<text text-anchor=\"start\" x=\"790.167\" y=\"-93.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 308</text>\n",
       "<text text-anchor=\"start\" x=\"792.667\" y=\"-78.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.604</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M647.768,-97.5C685.076,-97.5 733.96,-97.5 771.655,-97.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"771.972,-101 781.972,-97.5 771.972,-94.0001 771.972,-101\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"875.667,-53 785.667,-53 785.667,-0 875.667,-0 875.667,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"795.667\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.128</text>\n",
       "<text text-anchor=\"start\" x=\"793.667\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 20</text>\n",
       "<text text-anchor=\"start\" x=\"795.667\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 0.15</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M647.768,-80.7639C686.371,-69.2478 737.367,-54.0347 775.539,-42.6475\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"776.864,-45.9044 785.447,-39.6917 774.863,-39.1966 776.864,-45.9044\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f9c96aa5588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_tree(m.estimators_[0], df_trn, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we create a bigger tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.7129165449055029, 1.0, -1.0330056472379088]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set result looks great! But the validation set is worse than our original model. This is why we need to use *bagging* of multiple trees to get more generalizable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Intro to bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To learn about bagging in random forests, let's start with our basic model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22184454016270044, 0.5250063491679569, 0.8031398031398032, -0.10252972924924775]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll grab the predictions for each individual tree, and look at one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 1., 0., 1., 1., 0., 0., 1.]), 0.4, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\n",
    "preds[:,0], np.mean(preds[:,0]), y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 12000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c9697b048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The shape of this curve suggests that adding more trees isn't going to help us much. Let's check. (Compare this to our original model on a sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2042761611152902, 0.5127938182154694, 0.8330848330848332, -0.051832921758115935]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19398550203559028, 0.5060937001517934, 0.8494783494783494, -0.024526179239386936]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=80, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Out-of-bag (OOB) score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Is our validation set worse than our training set because we're over-fitting, or because the validation set is for a different time period, or a bit of both? With the existing information we've shown, we can't tell. However, random forests have a very clever trick called *out-of-bag (OOB) error* which can handle this (and more!)\n",
    "\n",
    "The idea is to calculate error on the training set, but only include the trees in the calculation of a row's error where that row was *not* included in training that tree. This allows us to see whether the model is over-fitting, without needing a separate validation set.\n",
    "\n",
    "This also has the benefit of allowing us to see whether our model generalizes, even if we only have a small amount of data so want to avoid separating some out to create a validation set.\n",
    "\n",
    "This is as simple as adding one more parameter to our model constructor. We print the OOB error last in our `print_score` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This shows that our validation set time difference is making an impact, as is model over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It turns out that one of the easiest ways to avoid over-fitting is also one of the best ways to speed up analysis: *subsampling*. Let's return to using our full dataset, so that we can demonstrate the impact of this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_trn, y_trn = proc_df(df_raw, 'SalePrice')\n",
    "X_train, X_valid = split_vals(df_trn, n_trn)\n",
    "y_train, y_valid = split_vals(y_trn, n_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The basic idea is this: rather than limit the total amount of data that our model can access, let's instead limit it to a *different* random subset per tree. That way, given enough trees, the model can still see *all* the data, but for each individual tree it'll be just as fast as if we had cut down our dataset as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set_rf_samples(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, oob_score=True)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since each additional tree allows the model to see more data, this approach can make additional trees more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tree building parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We revert to using a full bootstrap sample in order to show the impact of other over-fitting avoidance methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reset_rf_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's get a baseline for this full set to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another way to reduce over-fitting is to grow our trees less deeply. We do this by specifying (with `min_samples_leaf`) that we require some minimum number of rows in every leaf node. This has two benefits:\n",
    "\n",
    "- There are less decision rules for each leaf node; simpler models should generalize better\n",
    "- The predictions are made by averaging more rows in the leaf node, resulting in less volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also increase the amount of variation amongst the trees by not only use a sample of rows for each tree, but to also using a sample of *columns* for each *split*. We do this by specifying `max_features`, which is the proportion of features to randomly select from at each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- None\n",
    "- 0.5\n",
    "- 'sqrt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 1, 3, 5, 10, 25, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can't compare our results directly with the Kaggle competition, since it used a different validation set (and we can no longer to submit to this competition) - but we can at least see that we're getting similar results to the winners based on the dataset we have.\n",
    "\n",
    "The sklearn docs [show an example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html) of different `max_features` methods with increasing numbers of trees - as you see, using a subset of features on each split requires using more trees, but results in better models:\n",
    "![sklearn max_features chart](http://scikit-learn.org/stable/_images/sphx_glr_plot_ensemble_oob_001.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
